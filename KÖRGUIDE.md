# ğŸš€ KÃ–RGUIDE â€“ AI Study Mentor

VÃ¤lkommen till **AI Study Mentor** ğŸ‘‹  
En webbaserad studiementor dÃ¤r anvÃ¤ndaren kan ladda upp eget studiematerial (PDF) och chatta med en svensk AI-mentor som hjÃ¤lper till att fÃ¶rstÃ¥ innehÃ¥llet och skapa studiefrÃ¥gor.

---

## ğŸ§© FÃ¶rutsÃ¤ttningar

Innan du startar:

- Node.js **v18+**
- npm (ingÃ¥r i Node)
- Konto hos **MongoDB Atlas**
- Konto/API-nyckel fÃ¶r **Pinecone**
- **Ollama** installerat lokalt (Gemma3-4b-modellen)

---

## âš™ï¸ Installation och start

1. **Klona projektet**

   ```bash
   git clone https://github.com/<ditt-repo-namn>.git
   cd ai-study-mentor
   ```

2. **Installera beroenden**

   ```bash
   npm install


   ollama pull gemma3:4b
   ```

3. **Skapa miljÃ¶fil**
   Skapa en fil i projektets rotmapp:
   `.env.local`

   ```env

   MONGODB_URI="mongodb+srv://<User>:<Password>@cluster0.r6jzab0.mongodb.net/aiStudyMentorDatabase?retryWrites=true&w=majority&appName=Cluster0"
JWT_SECRET=<Your JWT Secret>
PINECONE_API_KEY=<Your Pinecone API Key>
NODE_ENV=development # change to 'production' in production

   ```

# MongoDB Atlas connection string

MONGODB_URI="mongodb+srv://anvandare:losenord@cluster.mongodb.net/ai-study-mentor"

# JWT secret for auth tokens

JWT_SECRET="byt-mig"

# Optional: NextAuth secret if we integrate next-auth later

NEXTAUTH_SECRET="byt-mig-ocksa"

````

4. **Starta utvecklingsservern**

```bash
npm run dev
````

5. **Ã–ppna appen**
   [http://localhost:3000](http://localhost:3000)

---

## ğŸ§  Funktioner

| Funktion                       | Beskrivning                                                |
| ------------------------------ | ---------------------------------------------------------- |
| ğŸ‘¤ **Autentisering**           | Registrera, logga in och logga ut anvÃ¤ndare (JWT, bcrypt)  |
| ğŸ—‚ï¸ **Dokumentuppladdning**     | Ladda upp PDF.                                             |
| ğŸ§¹ **Textutdragning**          | pdf-ts extraherar ren text                                 |
| ğŸ” **Semantisk sÃ¶kning (RAG)** | Pinecone hanterar embeddings och kontext                   |
| ğŸ¤– **AI-svar (Ollama)**        | Gemma 3-4b svarar kort pÃ¥ samma sprÃ¥k som frÃ¥gan stÃ¤lls pÃ¥ |
| ğŸ’¬ **ChatgrÃ¤nssnitt**          | Svensk UI med ljust/mÃ¶rkt lÃ¤ge och historik                |
| ğŸ§¾ **StudiefrÃ¥gor**            | Genererar 10 korta frÃ¥gor utifrÃ¥n innehÃ¥llet               |
| ğŸ’¾ **Databas**                 | MongoDB Atlas lagrar anvÃ¤ndare, dokument och sessioner     |

---

## ğŸ§ª SÃ¥ hÃ¤r testar du

1. **Registrera anvÃ¤ndare**
   Skicka `POST /api/auth/register` med JSON-data:

   ```json
   { "name": "Test", "email": "test@example.com", "password": "hemligt" }
   ```

2. **Logga in**
   `POST /api/auth/login` â†’ returnerar CSRF-token

3. **Ladda upp dokument**
   AnvÃ¤nd knappen **â€œLadda upp dokumentâ€** i UI:t
   (PDF)

4. **StÃ¤ll en frÃ¥ga**
   Exempel: _â€œVad handlar texten om?â€_
   â†’ AI svarar pÃ¥ svenska kort och koncist

5. **Generera studiefrÃ¥gor**
   AI skapar en lista med korta frÃ¥gor & tillhÃ¶rande svar

---

## ğŸ’¡ Vanliga problem

| Problem                    | Orsak / LÃ¶sning                                                   |
| -------------------------- | ----------------------------------------------------------------- |
| `MongoDB connection error` | Kontrollera `MONGODB_URI` och att IP Ã¤r vitlistad i Atlas         |
| `Pinecone index not found` | Skapa ett index i Pinecone-dashboard med samma namn som i `.env`  |
| `Ollama not responding`    | Kontrollera att Ollama-servern kÃ¶rs lokalt: `ollama serve`        |
| `JWT_SECRET missing`       | Se till att `JWT_SECRET` finns i `.env.local`                     |
| Chatten svarar inte        | Kontrollera API-nycklar och att Pinecone/Ollama-tjÃ¤nster Ã¤r igÃ¥ng |

---

## ğŸ“¦ Bygga fÃ¶r produktion

```bash
npm run build
npm start
```

Servern kÃ¶rs dÃ¥ pÃ¥ port **3000** (eller den du anger i miljÃ¶variabler).

---

## ğŸ Kort sammanfattning

```bash
git clone <repo>
npm install
cp .env.example .env.local
npm run dev
```

Ã–ppna â†’ [http://localhost:3000](http://localhost:3000)
Logga in â†’ Ladda upp â†’ StÃ¤ll en frÃ¥ga â†’ FÃ¥ svar baserat pÃ¥ innehÃ¥llet i uppladdat dokument âœ¨

```

```

## ğŸ§  Reflektion kring AI-komponenten

### Vilken ny AI-teknik/bibliotek identifierades och hur tillÃ¤mpades det?

Under projektet testade vi flera AI-verktyg och bibliotek fÃ¶r att jÃ¤mfÃ¶ra deras kapacitet att generera kod och driva applikationen:

* **Gemini (Google)** â€“ anvÃ¤ndes fÃ¶r att generera kod och visade sig ge mest robust och strukturerad kod utifrÃ¥n samma implementationsplan.
* **Codex (OpenAI)** â€“ fungerade bÃ¤ttre fÃ¶r att skapa UI och komponentlogik, men var mindre konsekvent i backend-hanteringen.
* **GitHub Copilot** â€“ anvÃ¤ndes som stÃ¶d vid mindre kodsnuttar, men inte som huvudkomponent.

I sjÃ¤lva applikationen implementerades:

* **Ollama + Gemma 3:4B** som lokal LLM-komponent fÃ¶r att analysera uppladdade dokument och svara pÃ¥ anvÃ¤ndarens frÃ¥gor.
* **Pinecone** som vektorbaserad databas i ett **RAG-flÃ¶de (Retrieval-Augmented Generation)** fÃ¶r att lagra och hÃ¤mta relevanta text-embeddingar. Detta gÃ¶r att modellen endast fÃ¥r den mest relevanta kontexten nÃ¤r den genererar svar.

### Motivering till val av teknik och bibliotek

Vi valde **Gemini** fÃ¶r utvecklingsfasen eftersom den producerade tydlig, sammanhÃ¤ngande och vÃ¤lorganiserad kod fÃ¶r hela projektet, sÃ¤rskilt vid komplexa integrationer mellan frontend och backend.

FÃ¶r den faktiska AI-funktionen valde vi **Ollama + Gemma 3:4B** eftersom:

* modellen kan kÃ¶ras **gratis och lokalt**, vilket gÃ¶r den idealisk under utveckling,
* den hanterar **stÃ¶rre dokument** pÃ¥ ett stabilt sÃ¤tt,
* den har **bra stÃ¶d fÃ¶r Pinecone**, vilket underlÃ¤ttar RAG-implementationen,
* och den **inte krÃ¤ver moln-API-kostnader**.

Vid fortsatt utveckling skulle man dock kunna uppgradera till en mer avancerad modell, som **GPT-4o** eller **Claude 3**, fÃ¶r att fÃ¶rbÃ¤ttra precision och svarskvalitet i en produktionsmiljÃ¶.

---

### VarfÃ¶r behÃ¶vdes AI-komponenten? Skulle ni kunna lÃ¶st det pÃ¥ ett annat sÃ¤tt?

AI-komponenten Ã¤r **helt nÃ¶dvÃ¤ndig** fÃ¶r att applikationen ska fungera som tÃ¤nkt. Systemets huvudsyfte Ã¤r att lÃ¥ta anvÃ¤ndaren ladda upp egna dokument (t.ex. PDF:er eller textfiler) och dÃ¤refter stÃ¤lla frÃ¥gor om innehÃ¥llet. FÃ¶r att kunna analysera text Ã¶ver flera sidor, fÃ¶rstÃ¥ sammanhang och ge korrekta, kontextuella svar krÃ¤vs **sprÃ¥klig fÃ¶rstÃ¥else och semantisk tolkning** â€” nÃ¥got som endast en AI-modell kan erbjuda.

Att fÃ¶rsÃ¶ka lÃ¶sa detta utan AI hade i praktiken inte varit mÃ¶jligt.
En traditionell lÃ¶sning, som exempelvis:

* enkel **text- eller nyckelordsÃ¶kning**,
* eller **regex-baserade filter**,
  hade bara kunnat hitta exakta ord eller fraser â€” inte fÃ¶rstÃ¥ meningen bakom anvÃ¤ndarens frÃ¥ga.

AI-komponenten (genom LLM + RAG) gÃ¶r dÃ¤remot att applikationen **fÃ¶rstÃ¥r betydelsen** av frÃ¥gan, **matchar rÃ¤tt kontext** ur dokumentet och **formulerar ett naturligt svar**.
Det Ã¤r dÃ¤rfÃ¶r inte realistiskt att ersÃ¤tta AI-delen med klassisk programmering om mÃ¥let Ã¤r att anvÃ¤ndaren ska kunna konversera fritt kring sitt eget material.

Kort sagt:
ğŸ‘‰ Utan AI hade applikationen bara kunnat **sÃ¶ka textstrÃ¤ngar**, men inte **fÃ¶rstÃ¥ innehÃ¥ll**.
Med AI blir det istÃ¤llet mÃ¶jligt att **analysera, resonera och svara som en mÃ¤nsklig studieassistent**.

---

Vill du att jag lÃ¤gger till en **kort inledande sammanfattning (2â€“3 meningar)** i toppen av denna reflektion â€” t.ex. en beskrivning av syftet med att anvÃ¤nda AI i projektet â€” sÃ¥ README:n fÃ¥r en mer berÃ¤ttande ton?

