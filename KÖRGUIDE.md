# ğŸš€ KÃ–RGUIDE â€“ AI Study Mentor

VÃ¤lkommen till **AI Study Mentor** ğŸ‘‹  
En webbaserad studiementor dÃ¤r anvÃ¤ndaren kan ladda upp eget studiematerial (PDF) och chatta med en svensk AI-mentor som hjÃ¤lper till att fÃ¶rstÃ¥ innehÃ¥llet och skapa studiefrÃ¥gor.

---

## ğŸ§© FÃ¶rutsÃ¤ttningar

Innan du startar:

- Node.js **v18+**
- npm (ingÃ¥r i Node)
- Konto hos **MongoDB Atlas**
- Konto/API-nyckel fÃ¶r **Pinecone**
- **Ollama** installerat lokalt (Gemma3-4b-modellen)

---

## âš™ï¸ Installation och start

1. **Klona projektet**

   ```bash
   git clone https://github.com/<ditt-repo-namn>.git
   cd ai-study-mentor
   ```

2. **Installera beroenden**

   ```bash
   npm install


   ollama pull gemma3:4b
   ```

3. **Skapa miljÃ¶fil**
   Skapa en fil i projektets rotmapp:
   ```bash
   MONGODB_URI="mongodb+srv://anvandare:losenord@cluster.mongodb.net/ai-study-mentor"
   JWT_SECRET="byt-mig"  
   NEXTAUTH_SECRET="byt-mig-ocksa"
   ````

5. **Starta utvecklingsservern**

```bash
npm run dev
````

5. **Ã–ppna appen**
   [http://localhost:3000](http://localhost:3000)

---

## ğŸ§  Funktioner

| Funktion                       | Beskrivning                                                |
| ------------------------------ | ---------------------------------------------------------- |
| ğŸ‘¤ **Autentisering**           | Registrera, logga in och logga ut anvÃ¤ndare (JWT, bcrypt)  |
| ğŸ—‚ï¸ **Dokumentuppladdning**     | Ladda upp PDF.                                             |
| ğŸ§¹ **Textutdragning**          | pdf-ts extraherar ren text                                 |
| ğŸ” **Semantisk sÃ¶kning (RAG)** | Pinecone hanterar embeddings och kontext                   |
| ğŸ¤– **AI-svar (Ollama)**        | Gemma 3-4b svarar kort pÃ¥ samma sprÃ¥k som frÃ¥gan stÃ¤lls pÃ¥ |
| ğŸ’¬ **ChatgrÃ¤nssnitt**          | Svensk UI med ljust/mÃ¶rkt lÃ¤ge och historik                |
| ğŸ§¾ **StudiefrÃ¥gor**            | Genererar 10 korta frÃ¥gor och svar utifrÃ¥n innehÃ¥llet      |
| ğŸ’¾ **Databas**                 | MongoDB Atlas lagrar anvÃ¤ndare, dokument och sessioner     |

---

## ğŸ§ª SÃ¥ hÃ¤r testar du

1. **Registrera anvÃ¤ndare**
   Skicka `POST /api/auth/register` med JSON-data:

   ```json
   { "name": "Test", "email": "test@example.com", "password": "hemligt" }
   ```

2. **Logga in**
   `POST /api/auth/login` â†’ returnerar CSRF-token

3. **Ladda upp dokument**
   AnvÃ¤nd knappen **â€œLadda upp dokumentâ€** i UI:t
   (PDF)

4. **StÃ¤ll en frÃ¥ga**
   Exempel: _â€œVad handlar texten om?â€_
   â†’ AI svarar pÃ¥ svenska kort och koncist

5. **Generera studiefrÃ¥gor**
   AI skapar en lista med korta frÃ¥gor & tillhÃ¶rande svar

---

## ğŸ’¡ Vanliga problem

| Problem                    | Orsak / LÃ¶sning                                                   |
| -------------------------- | ----------------------------------------------------------------- |
| `MongoDB connection error` | Kontrollera `MONGODB_URI` och att IP Ã¤r vitlistad i Atlas         |
| `Ollama not responding`    | Kontrollera att Ollama-servern kÃ¶rs lokalt: `ollama serve`        |
| `JWT_SECRET missing`       | Se till att `JWT_SECRET` finns i `.env.local`                     |
| Chatten svarar inte        | Kontrollera API-nycklar och att Pinecone/Ollama-tjÃ¤nster Ã¤r igÃ¥ng |

---

## ğŸ“¦ Bygga fÃ¶r produktion

```bash
npm run build
npm start
```

Servern kÃ¶rs dÃ¥ pÃ¥ port **3000** (eller den du anger i miljÃ¶variabler).

---

## ğŸ Kort sammanfattning

```bash
git clone <repo>
npm install
cp .env.example .env.local
npm run dev
```

Ã–ppna â†’ [http://localhost:3000](http://localhost:3000)
Logga in â†’ Ladda upp â†’ StÃ¤ll en frÃ¥ga â†’ FÃ¥ svar baserat pÃ¥ innehÃ¥llet i uppladdat dokument âœ¨

```

```

ğŸ§  Reflektion kring AI-komponenten

### Vilken ny AI-teknik/bibliotek identifierades och hur tillÃ¤mpades det?

Under projektet testade vi flera AI-verktyg och bibliotek fÃ¶r att jÃ¤mfÃ¶ra deras kapacitet att bÃ¥de generera kod och driva sjÃ¤lva applikationen:

#### Gemini CLI (Google) 
â€“ anvÃ¤ndes fÃ¶r kodgenerering och visade sig ge mest robust och vÃ¤lstrukturerad kod utifrÃ¥n samma implementationsplan.

#### Codex (OpenAI) 
â€“ presterade bÃ¤ttre vid skapandet av UI-komponenter, men saknade stabilitet och helhet jÃ¤mfÃ¶rt med Gemini. AnvÃ¤ndes dock i slutversionen. 

#### GitHub Copilot 
â€“ anvÃ¤ndes som kodstÃ¶d vid mindre moment, men inte som huvudmotor.

I den fÃ¤rdiga applikationen implementerades tre centrala AI-komponenter:

#### Ollama + Gemma 3:4B
â€“ Lokalt kÃ¶rd LLM (Large Language Model) fÃ¶r att analysera uppladdade dokument och besvara anvÃ¤ndarens frÃ¥gor.
â€“ Valdes fÃ¶r sin hÃ¶ga prestanda, enkel lokalintegration och att den kan kÃ¶ras helt kostnadsfritt under utveckling.

#### Pinecone
â€“ AnvÃ¤nds som vektorbaserad databas i ett RAG-flÃ¶de (Retrieval-Augmented Generation).
â€“ Lagrar embeddings av dokumentens textstycken och hÃ¤mtar de mest relevanta delarna nÃ¤r anvÃ¤ndaren stÃ¤ller en frÃ¥ga, vilket ger AI:n rÃ¤tt kontext.

#### Hugging Face Transformers Embeddings
â€“ Denna modell anvÃ¤ndes fÃ¶r att skapa semantiska text-embeddingar av dokumenten innan de skickades till Pinecone.
â€“ all-MiniLM-L6-v2 valdes eftersom den Ã¤r lÃ¤ttviktig, snabb, gratis och erbjuder bra balans mellan noggrannhet och prestanda vid semantisk sÃ¶kning.
â€“ Kombinationen av denna embeddings-modell och Pinecone gjorde RAG-lÃ¶sningen bÃ¥de effektiv och resurssnÃ¥l.



### Motivering till val av teknik och bibliotek

Vi valde Gemini fÃ¶r utvecklingsfasen eftersom den producerade den mest konsekventa och lÃ¤sbara koden, sÃ¤rskilt vid integration mellan frontend och backend.

FÃ¶r AI-komponenten valdes Ollama + Gemma 3:4B eftersom:
den kan kÃ¶ras gratis och lokalt, perfekt under utveckling,
den hanterar stÃ¶rre dokument stabilt,
den har bra kompatibilitet med Pinecone,
och den krÃ¤ver inga externa API-kostnader.

FÃ¶r textfÃ¶rstÃ¥else och sÃ¶kbarhet anvÃ¤ndes Hugging Face MiniLM-modellen eftersom den tillfÃ¶rde semantisk sÃ¶kfunktionalitet som var helt nÃ¶dvÃ¤ndig fÃ¶r att RAG-arkitekturen skulle fungera korrekt.

Vid fortsatt utveckling eller kommersiell lansering kan dessa komponenter enkelt bytas ut mot mer avancerade alternativ som OpenAI GPT-4o eller Claude, vilket skulle fÃ¶rbÃ¤ttra precision och svarskvalitet ytterligare.



### VarfÃ¶r behÃ¶vdes AI-komponenten? Skulle ni kunna lÃ¶st det pÃ¥ ett annat sÃ¤tt?

AI-komponenten Ã¤r **helt nÃ¶dvÃ¤ndig** fÃ¶r att applikationen ska fungera som tÃ¤nkt. Systemets huvudsyfte Ã¤r att lÃ¥ta anvÃ¤ndaren ladda upp egna dokument (t.ex. PDF:er eller textfiler) och dÃ¤refter stÃ¤lla frÃ¥gor om innehÃ¥llet. FÃ¶r att kunna analysera text Ã¶ver flera sidor, fÃ¶rstÃ¥ sammanhang och ge korrekta, kontextuella svar krÃ¤vs **sprÃ¥klig fÃ¶rstÃ¥else och semantisk tolkning** nÃ¥got som endast en AI-modell kan erbjuda.

Att fÃ¶rsÃ¶ka lÃ¶sa detta utan AI hade i praktiken inte varit mÃ¶jligt.
En traditionell lÃ¶sning, som exempelvis:

* enkel **text- eller nyckelordsÃ¶kning**,
* eller **regex-baserade filter**,
  hade bara kunnat hitta exakta ord eller fraser och inte fÃ¶rstÃ¥ meningen bakom anvÃ¤ndarens frÃ¥ga.

AI-komponenten (genom LLM + RAG) gÃ¶r dÃ¤remot att applikationen **fÃ¶rstÃ¥r betydelsen** av frÃ¥gan, **matchar rÃ¤tt kontext** ur dokumentet och **formulerar ett naturligt svar**.
Det Ã¤r dÃ¤rfÃ¶r inte realistiskt att ersÃ¤tta AI-delen med klassisk programmering om mÃ¥let Ã¤r att anvÃ¤ndaren ska kunna konversera fritt kring sitt eget material.

Utan AI hade applikationen bara kunnat **sÃ¶ka textstrÃ¤ngar**, men inte **fÃ¶rstÃ¥ innehÃ¥ll**.
Med AI blir det istÃ¤llet mÃ¶jligt att **analysera, resonera och svara som en mÃ¤nsklig studieassistent**.


